#!/usr/bin/env python3
"""
RSS Feed Fetcher - å¯åŠ¨æ—¶è‡ªåŠ¨æŠ“å–ç§‘æŠ€æº
"""

import os
# SSL FIX APPLIED
import ssl
ssl._create_default_https_context = ssl._create_unverified_context

import json
import feedparser
from datetime import datetime
from pathlib import Path

# ============== é…ç½® ==============
WORKSPACE = "C:/Users/æ®‡/.openclaw/workspace"
RSS_DATA = f"{WORKSPACE}/rss_feed.json"
RSS_LOG = f"{WORKSPACE}/rss_fetch.log"

# ç§‘æŠ€ RSS æº
RSS_FEEDS = [
    {
        "name": "TechCrunch",
        "url": "https://techcrunch.com/feed/",
        "category": "ç§‘æŠ€"
    },
    {
        "name": "The Verge",
        "url": "https://www.theverge.com/rss/index.xml",
        "category": "ç§‘æŠ€"
    },
    {
        "name": "Wired",
        "url": "https://www.wired.com/feed/rss",
        "category": "ç§‘æŠ€"
    },
    {
        "name": "MIT Technology Review",
        "url": "https://www.technologyreview.com/feed/",
        "category": "ç§‘æŠ€"
    },
    {
        "name": "Hacker News",
        "url": "https://hnrss.org/frontpage",
        "category": "ç§‘æŠ€"
    },
    {
        "name": "Product Hunt",
        "url": "https://www.producthunt.com/feed",
        "category": "äº§å“"
    },
    {
        "name": "GitHub Blog",
        "url": "https://github.blog/feed/",
        "category": "å¼€å‘"
    },
    {
        "name": "OpenAI Blog",
        "url": "https://openai.com/blog/rss.xml",
        "category": "AI"
    }
]

class RSSFetcher:
    """RSS æŠ“å–å™¨"""
    
    def __init__(self):
        self.feeds = RSS_FEEDS
        self.results = []
        self.start_time = datetime.now()
    
    def log(self, message):
        """æ—¥å¿—è®°å½•"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        log_entry = f"[{timestamp}] {message}\n"
        
        with open(RSS_LOG, 'a', encoding='utf-8', errors='ignore') as f:
            f.write(log_entry)
        
        # ç§»é™¤ emoji é¿å…ç¼–ç é—®é¢˜
        clean_message = message.replace("âœ…", "[OK]").replace("âŒ", "[FAIL]").replace("âš ï¸", "[WARN]")
        print(f"[{timestamp}] {clean_message}")
    
    def fetch_feed(self, feed):
        """æŠ“å–å•ä¸ª RSS æº"""
        try:
            self.log(f"æŠ“å–: {feed['name']}...")
            
            # è§£æ RSS
            parsed = feedparser.parse(feed['url'])
            
            if parsed.bozo:
                self.log(f"  [WARN] è§£æé”™è¯¯: {parsed.bozo_exception}")
                return None
            
            # æå–æ–‡ç« 
            articles = []
            for entry in parsed.entries[:5]:  # æ¯ä¸ªæºå–å‰5æ¡
                article = {
                    "title": entry.get("title", ""),
                    "link": entry.get("link", ""),
                    "published": entry.get("published", ""),
                    "summary": entry.get("summary", "")[:200],
                    "source": feed['name'],
                    "category": feed['category'],
                    "fetched_at": datetime.now().isoformat()
                }
                articles.append(article)
            
            self.log(f"  [OK] è·å– {len(articles)} ç¯‡æ–‡ç« ")
            
            return {
                "source": feed['name'],
                "category": feed['category'],
                "url": feed['url'],
                "articles": articles,
                "fetched_at": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.log(f"  [FAIL] å¤±è´¥: {e}")
            return None
    
    def fetch_all(self):
        """æŠ“å–æ‰€æœ‰ RSS æº"""
        self.log("="*60)
        self.log("RSS Feed Fetcher - ç§‘æŠ€æºæŠ“å–")
        self.log(f"å¯åŠ¨æ—¶é—´: {self.start_time.isoformat()}")
        self.log(f"æºæ•°é‡: {len(self.feeds)}")
        self.log("="*60)
        
        results = []
        
        for feed in self.feeds:
            result = self.fetch_feed(feed)
            if result:
                results.append(result)
        
        # ä¿å­˜ç»“æœ
        self.save_results(results)
        
        # å®Œæˆ
        end_time = datetime.now()
        duration = (end_time - self.start_time).total_seconds()
        
        self.log("="*60)
        self.log(f"æŠ“å–å®Œæˆ! è€—æ—¶: {duration:.2f}ç§’")
        self.log(f"æˆåŠŸ: {len(results)}/{len(self.feeds)} ä¸ªæº")
        self.log(f"æ–‡ç« æ€»æ•°: {sum(len(r['articles']) for r in results)}")
        self.log("="*60)
        
        return results
    
    def save_results(self, results):
        """ä¿å­˜æŠ“å–ç»“æœ"""
        data = {
            "fetched_at": datetime.now().isoformat(),
            "sources_count": len(results),
            "total_articles": sum(len(r['articles']) for r in results),
            "feeds": results
        }
        
        with open(RSS_DATA, 'w', encoding='utf-8', errors='ignore') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        self.log(f"ç»“æœå·²ä¿å­˜åˆ°: {RSS_DATA}")
    
    def get_summary(self):
        """è·å–æ‘˜è¦"""
        if not os.path.exists(RSS_DATA):
            return None
        
        with open(RSS_DATA, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        summary = f"""
ğŸ“° RSS ç§‘æŠ€æºæ‘˜è¦
æŠ“å–æ—¶é—´: {data['fetched_at']}
æºæ•°é‡: {data['sources_count']}
æ–‡ç« æ€»æ•°: {data['total_articles']}

æœ€æ–°æ–‡ç« :
"""
        
        for feed in data['feeds'][:3]:  # æ˜¾ç¤ºå‰3ä¸ªæº
            summary += f"\n[{feed['source']}]\n"
            for article in feed['articles'][:3]:  # æ¯ä¸ªæºæ˜¾ç¤ºå‰3æ¡
                summary += f"  â€¢ {article['title'][:50]}...\n"
                summary += f"    {article['link']}\n"
        
        return summary


def main():
    """ä¸»å…¥å£"""
    fetcher = RSSFetcher()
    results = fetcher.fetch_all()
    return results


if __name__ == "__main__":
    main()
